{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teWC8XjO7rhx"
      },
      "outputs": [],
      "source": [
        "!pip install -q diffusers transformers xformers accelerate\n",
        "!pip install -q numpy scipy ftfy Pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import time\n",
        "\n",
        "from PIL import Image\n",
        "from IPython import display as IPdisplay\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import (\n",
        "    DDIMScheduler,\n",
        "    PNDMScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        ")\n",
        "from transformers import logging\n",
        "\n",
        "logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "k-W82IDj7sGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True"
      ],
      "metadata": {
        "id": "5QkS1D8c7vIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    scheduler=scheduler,\n",
        "    torch_dtype=torch.float32,\n",
        ").to(device)\n",
        "\n",
        "# Disable image generation progress bar, we'll display our own\n",
        "pipe.set_progress_bar_config(disable=True)"
      ],
      "metadata": {
        "id": "v1WQk0n77yd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Offloading the weights to the CPU and only loading them on the GPU can reduce memory consumption to less than 3GB.\n",
        "pipe.enable_model_cpu_offload()\n",
        "\n",
        "# Tighter ordering of memory tensors.\n",
        "pipe.unet.to(memory_format=torch.channels_last)\n",
        "\n",
        "# Decoding large batches of images with limited VRAM or batches with 32 images or more by decoding the batches of latents one image at a time.\n",
        "pipe.enable_vae_slicing()\n",
        "\n",
        "# Splitting the image into overlapping tiles, decoding the tiles, and then blending the outputs together to compose the final image.\n",
        "pipe.enable_vae_tiling()\n",
        "\n",
        "# Using Flash Attention; If you have PyTorch >= 2.0 installed, you should not expect a speed-up for inference when enabling xformers.\n",
        "pipe.enable_xformers_memory_efficient_attention()"
      ],
      "metadata": {
        "id": "m-VtHK1d70Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_images(images, save_path):\n",
        "    try:\n",
        "        # Convert each image in the 'images' list from an array to an Image object.\n",
        "        images = [\n",
        "            Image.fromarray(np.array(image[0], dtype=np.uint8)) for image in images\n",
        "        ]\n",
        "\n",
        "        # Generate a file name based on the current time, replacing colons with hyphens\n",
        "        # to ensure the filename is valid for file systems that don't allow colons.\n",
        "        filename = (\n",
        "            time.strftime(\"%H:%M:%S\", time.localtime())\n",
        "            .replace(\":\", \"-\")\n",
        "        )\n",
        "        # Save the first image in the list as a GIF file at the 'save_path' location.\n",
        "        # The rest of the images in the list are added as subsequent frames to the GIF.\n",
        "        # The GIF will play each frame for 100 milliseconds and will loop indefinitely.\n",
        "        images[0].save(\n",
        "            f\"{save_path}/{filename}.gif\",\n",
        "            save_all=True,\n",
        "            append_images=images[1:],\n",
        "            duration=100,\n",
        "            loop=0,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # If there is an error during the process, print the exception message.\n",
        "        print(e)\n",
        "\n",
        "    # Return the saved GIF as an IPython display object so it can be displayed in a notebook.\n",
        "    return IPdisplay.Image(f\"{save_path}/{filename}.gif\")"
      ],
      "metadata": {
        "id": "Wgv5Gmff71nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The seed is set to \"None\", because we want different results each time we run the generation.\n",
        "seed = None\n",
        "\n",
        "if seed is not None:\n",
        "    generator = torch.manual_seed(seed)\n",
        "else:\n",
        "    generator = None\n",
        "\n",
        "# The guidance scale is set to its normal range (7 - 10).\n",
        "guidance_scale = 8\n",
        "\n",
        "# The number of inference steps was chosen empirically to generate an acceptable picture within an acceptable time.\n",
        "num_inference_steps = 15\n",
        "\n",
        "# The higher you set this value, the smoother the interpolations will be. However, the generation time will increase. This value was chosen empirically.\n",
        "num_interpolation_steps = 30\n",
        "\n",
        "# I would not recommend less than 512 on either dimension. This is because this model was trained on 512x512 image resolution.\n",
        "height = 512\n",
        "width = 512\n",
        "\n",
        "# The path where the generated GIFs will be saved\n",
        "save_path = \"/output\"\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)"
      ],
      "metadata": {
        "id": "Y-3ZveBt74C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The text prompt that describes the desired output image.\n",
        "prompt = \"Epic shot of Sweden, ultra detailed lake with an ren dear, nostalgic vintage, ultra cozy and inviting, wonderful light atmosphere, fairy, little photorealistic, digital painting, sharp focus, ultra cozy and inviting, wish to be there. very detailed, arty, should rank high on youtube for a dream trip.\"\n",
        "# A negative prompt that can be used to steer the generation away from certain features; here, it is empty.\n",
        "negative_prompt = \"poorly drawn,cartoon, 2d, disfigured, bad art, deformed, poorly drawn, extra limbs, close up, b&w, weird colors, blurry\"\n",
        "\n",
        "# The step size for the interpolation in the latent space.\n",
        "step_size = 0.001\n",
        "\n",
        "# Tokenizing and encoding the prompt into embeddings.\n",
        "prompt_tokens = pipe.tokenizer(\n",
        "    prompt,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "prompt_embeds = pipe.text_encoder(prompt_tokens.input_ids.to(device))[0]\n",
        "\n",
        "\n",
        "# Tokenizing and encoding the negative prompt into embeddings.\n",
        "if negative_prompt is None:\n",
        "    negative_prompt = [\"\"]\n",
        "\n",
        "negative_prompt_tokens = pipe.tokenizer(\n",
        "    negative_prompt,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "negative_prompt_embeds = pipe.text_encoder(negative_prompt_tokens.input_ids.to(device))[0]"
      ],
      "metadata": {
        "id": "F2gdlWGQ76C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating initial latent vectors from a random normal distribution, with the option to use a generator for reproducibility.\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        ")\n",
        "\n",
        "walked_embeddings = []\n",
        "\n",
        "# Interpolating between embeddings for the given number of interpolation steps.\n",
        "for i in range(num_interpolation_steps):\n",
        "    walked_embeddings.append(\n",
        "        [prompt_embeds + step_size * i, negative_prompt_embeds + step_size * i]\n",
        "    )"
      ],
      "metadata": {
        "id": "NKvqY15C77zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating images using the interpolated embeddings.\n",
        "images = []\n",
        "for latent in tqdm(walked_embeddings):\n",
        "    images.append(\n",
        "        pipe(\n",
        "            height=height,\n",
        "            width=width,\n",
        "            num_images_per_prompt=1,\n",
        "            prompt_embeds=latent[0],\n",
        "            negative_prompt_embeds=latent[1],\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=generator,\n",
        "            latents=latents,\n",
        "        ).images\n",
        "    )\n",
        "\n",
        "# Display of saved generated images.\n",
        "display_images(images, save_path)"
      ],
      "metadata": {
        "id": "-jX9tKwq79Ud"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}