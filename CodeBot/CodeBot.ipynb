{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets peft bitsandbytes flash-attn"
      ],
      "metadata": {
        "id": "WIYLZgMSqMP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLUH4StwqGNM"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import functools\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import random\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from peft.tuners.lora import LoraLayer\n",
        "from peft import PeftModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL=\"bigcode/starcoderbase-1b\" # Model checkpoint on the Hugging Face Hub\n",
        "DATASET=\"smangrul/hf-stack-v1\"   # Dataset on the Hugging Face Hub\n",
        "DATA_COLUMN=\"content\"            # Column name containing the code content\n",
        "\n",
        "SEQ_LENGTH=2048                  # Sequence length\n",
        "\n",
        "# Training arguments\n",
        "MAX_STEPS=2000                   # max_steps\n",
        "BATCH_SIZE=16                    # batch_size\n",
        "GR_ACC_STEPS=1                   # gradient_accumulation_steps\n",
        "LR=5e-4                          # learning_rate\n",
        "LR_SCHEDULER_TYPE=\"cosine\"       # lr_scheduler_type\n",
        "WEIGHT_DECAY=0.01                # weight_decay\n",
        "NUM_WARMUP_STEPS=30              # num_warmup_steps\n",
        "EVAL_FREQ=100                    # eval_freq\n",
        "SAVE_FREQ=100                    # save_freq\n",
        "LOG_FREQ=25                      # log_freq\n",
        "OUTPUT_DIR=\"sample-starcoder\" # output_dir\n",
        "BF16=True                        # bf16\n",
        "FP16=False                       # no_fp16\n",
        "\n",
        "# FIM trasformations arguments\n",
        "FIM_RATE=0.5                     # fim_rate\n",
        "FIM_SPM_RATE=0.5                 # fim_spm_rate\n",
        "\n",
        "# LORA\n",
        "LORA_R=8                         # lora_r\n",
        "LORA_ALPHA=32                    # lora_alpha\n",
        "LORA_DROPOUT=0.0                 # lora_dropout\n",
        "LORA_TARGET_MODULES=\"c_proj,c_attn,q_attn,c_fc,c_proj\"    # lora_target_modules\n",
        "\n",
        "# bitsandbytes config\n",
        "USE_NESTED_QUANT=True            # use_nested_quant\n",
        "BNB_4BIT_COMPUTE_DTYPE=\"bfloat16\"# bnb_4bit_compute_dtype\n",
        "\n",
        "SEED=0"
      ],
      "metadata": {
        "id": "ygdNKYWEqP_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    logging,\n",
        "    set_seed,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "set_seed(SEED)"
      ],
      "metadata": {
        "id": "MP8zxLjiqRpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\n",
        "    DATASET,\n",
        "    data_dir=\"data\",\n",
        "    split=\"train\",\n",
        "    streaming=True,\n",
        ")\n",
        "\n",
        "valid_data = dataset.take(4000)\n",
        "train_data = dataset.skip(4000)\n",
        "train_data = train_data.shuffle(buffer_size=5000, seed=SEED)"
      ],
      "metadata": {
        "id": "ZA_X87HVqV1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
        "\n",
        "def chars_token_ratio(dataset, tokenizer, data_column, nb_examples=400):\n",
        "    \"\"\"\n",
        "    Estimate the average number of characters per token in the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    total_characters, total_tokens = 0, 0\n",
        "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
        "        total_characters += len(example[data_column])\n",
        "        total_tokens += len(tokenizer(example[data_column]).tokens())\n",
        "\n",
        "    return total_characters / total_tokens\n",
        "\n",
        "\n",
        "chars_per_token = chars_token_ratio(train_data, tokenizer, DATA_COLUMN)\n",
        "print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\") # The character-to-token ratio can also be used as an indicator of the quality of text tokenization - between 2.0 and 3.5 can be considered good enough"
      ],
      "metadata": {
        "id": "wOsB4t5UqXaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to get token ids of the special tokens for prefix, suffix and middle for FIM transformations.\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def get_fim_token_ids(tokenizer):\n",
        "    try:\n",
        "        FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX, FIM_PAD = tokenizer.special_tokens_map[\"additional_special_tokens\"][1:5]\n",
        "        suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id = (\n",
        "            tokenizer.vocab[tok] for tok in [FIM_SUFFIX, FIM_PREFIX, FIM_MIDDLE, FIM_PAD]\n",
        "        )\n",
        "    except KeyError:\n",
        "        suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id = None, None, None, None\n",
        "    return suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id\n",
        "\n",
        "\n",
        "## Adapted from https://github.com/bigcode-project/Megatron-LM/blob/6c4bf908df8fd86b4977f54bf5b8bd4b521003d1/megatron/data/gpt_dataset.py\n",
        "def permute(\n",
        "    sample,\n",
        "    np_rng,\n",
        "    suffix_tok_id,\n",
        "    prefix_tok_id,\n",
        "    middle_tok_id,\n",
        "    pad_tok_id,\n",
        "    fim_rate=0.5,\n",
        "    fim_spm_rate=0.5,\n",
        "    truncate_or_pad=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Take in a sample (list of tokens) and perform a FIM transformation on it with a probability of fim_rate, using two FIM modes:\n",
        "    PSM and SPM (with a probability of fim_spm_rate).\n",
        "    \"\"\"\n",
        "\n",
        "    # The if condition will trigger with the probability of fim_rate\n",
        "    # This means FIM transformations will apply to samples with a probability of fim_rate\n",
        "    if np_rng.binomial(1, fim_rate):\n",
        "\n",
        "        # Split the sample into prefix, middle, and suffix, based on randomly generated indices stored in the boundaries list.\n",
        "        boundaries = list(np_rng.randint(low=0, high=len(sample) + 1, size=2))\n",
        "        boundaries.sort()\n",
        "\n",
        "        prefix = np.array(sample[: boundaries[0]], dtype=np.int64)\n",
        "        middle = np.array(sample[boundaries[0] : boundaries[1]], dtype=np.int64)\n",
        "        suffix = np.array(sample[boundaries[1] :], dtype=np.int64)\n",
        "\n",
        "        if truncate_or_pad:\n",
        "            # calculate the new total length of the sample, taking into account tokens indicating prefix, middle, and suffix\n",
        "            new_length = suffix.shape[0] + prefix.shape[0] + middle.shape[0] + 3\n",
        "            diff = new_length - len(sample)\n",
        "\n",
        "            # trancate or pad if there's a difference in length between the new length and the original\n",
        "            if diff > 0:\n",
        "                if suffix.shape[0] <= diff:\n",
        "                    return sample, np_rng\n",
        "                suffix = suffix[: suffix.shape[0] - diff]\n",
        "            elif diff < 0:\n",
        "                suffix = np.concatenate([suffix, np.full((-1 * diff), pad_tok_id)])\n",
        "\n",
        "        # With the probability of fim_spm_rateapply SPM variant of FIM transformations\n",
        "        # SPM: suffix, prefix, middle\n",
        "        if np_rng.binomial(1, fim_spm_rate):\n",
        "            new_sample = np.concatenate(\n",
        "                [\n",
        "                    [prefix_tok_id, suffix_tok_id],\n",
        "                    suffix,\n",
        "                    [middle_tok_id],\n",
        "                    prefix,\n",
        "                    middle,\n",
        "                ]\n",
        "            )\n",
        "        # Otherwise, apply the PSM variant of FIM transformations\n",
        "        # PSM: prefix, suffix, middle\n",
        "        else:\n",
        "\n",
        "            new_sample = np.concatenate(\n",
        "                [\n",
        "                    [prefix_tok_id],\n",
        "                    prefix,\n",
        "                    [suffix_tok_id],\n",
        "                    suffix,\n",
        "                    [middle_tok_id],\n",
        "                    middle,\n",
        "                ]\n",
        "            )\n",
        "    else:\n",
        "        # don't apply FIM transformations\n",
        "        new_sample = sample\n",
        "\n",
        "    return list(new_sample), np_rng"
      ],
      "metadata": {
        "id": "Qs-Qmy0uqhNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Iterable dataset that returns constant-length chunks of tokens from a stream of text files.\n",
        "\n",
        "class ConstantLengthDataset(IterableDataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        dataset,\n",
        "        infinite=False,\n",
        "        seq_length=1024,\n",
        "        num_of_sequences=1024,\n",
        "        chars_per_token=3.6,\n",
        "        content_field=\"content\",\n",
        "        fim_rate=0.5,\n",
        "        fim_spm_rate=0.5,\n",
        "        seed=0,\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.concat_token_id = tokenizer.eos_token_id\n",
        "        self.dataset = dataset\n",
        "        self.seq_length = seq_length\n",
        "        self.infinite = infinite\n",
        "        self.current_size = 0\n",
        "        self.max_buffer_size = seq_length * chars_per_token * num_of_sequences\n",
        "        self.content_field = content_field\n",
        "        self.fim_rate = fim_rate\n",
        "        self.fim_spm_rate = fim_spm_rate\n",
        "        self.seed = seed\n",
        "\n",
        "        (\n",
        "            self.suffix_tok_id,\n",
        "            self.prefix_tok_id,\n",
        "            self.middle_tok_id,\n",
        "            self.pad_tok_id,\n",
        "        ) = get_fim_token_ids(self.tokenizer)\n",
        "        if not self.suffix_tok_id and self.fim_rate > 0:\n",
        "            print(\"FIM is not supported by tokenizer, disabling FIM\")\n",
        "            self.fim_rate = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        iterator = iter(self.dataset)\n",
        "        more_examples = True\n",
        "        np_rng = np.random.RandomState(seed=self.seed)\n",
        "        while more_examples:\n",
        "            buffer, buffer_len = [], 0\n",
        "            while True:\n",
        "                if buffer_len >= self.max_buffer_size:\n",
        "                    break\n",
        "                try:\n",
        "                    buffer.append(next(iterator)[self.content_field])\n",
        "                    buffer_len += len(buffer[-1])\n",
        "                except StopIteration:\n",
        "                    if self.infinite:\n",
        "                        iterator = iter(self.dataset)\n",
        "                    else:\n",
        "                        more_examples = False\n",
        "                        break\n",
        "            tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n",
        "            all_token_ids = []\n",
        "\n",
        "            for tokenized_input in tokenized_inputs:\n",
        "                # optionally do FIM permutations\n",
        "                if self.fim_rate > 0:\n",
        "                    tokenized_input, np_rng = permute(\n",
        "                        tokenized_input,\n",
        "                        np_rng,\n",
        "                        self.suffix_tok_id,\n",
        "                        self.prefix_tok_id,\n",
        "                        self.middle_tok_id,\n",
        "                        self.pad_tok_id,\n",
        "                        fim_rate=self.fim_rate,\n",
        "                        fim_spm_rate=self.fim_spm_rate,\n",
        "                        truncate_or_pad=False,\n",
        "                    )\n",
        "\n",
        "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
        "            examples = []\n",
        "            for i in range(0, len(all_token_ids), self.seq_length):\n",
        "                input_ids = all_token_ids[i : i + self.seq_length]\n",
        "                if len(input_ids) == self.seq_length:\n",
        "                    examples.append(input_ids)\n",
        "            random.shuffle(examples)\n",
        "            for example in examples:\n",
        "                self.current_size += 1\n",
        "                yield {\n",
        "                    \"input_ids\": torch.LongTensor(example),\n",
        "                    \"labels\": torch.LongTensor(example),\n",
        "                }\n",
        "\n",
        "\n",
        "train_dataset = ConstantLengthDataset(\n",
        "        tokenizer,\n",
        "        train_data,\n",
        "        infinite=True,\n",
        "        seq_length=SEQ_LENGTH,\n",
        "        chars_per_token=chars_per_token,\n",
        "        content_field=DATA_COLUMN,\n",
        "        fim_rate=FIM_RATE,\n",
        "        fim_spm_rate=FIM_SPM_RATE,\n",
        "        seed=SEED,\n",
        ")\n",
        "eval_dataset = ConstantLengthDataset(\n",
        "        tokenizer,\n",
        "        valid_data,\n",
        "        infinite=False,\n",
        "        seq_length=SEQ_LENGTH,\n",
        "        chars_per_token=chars_per_token,\n",
        "        content_field=DATA_COLUMN,\n",
        "        fim_rate=FIM_RATE,\n",
        "        fim_spm_rate=FIM_SPM_RATE,\n",
        "        seed=SEED,\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "5cgCIaA_r50P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_in_8bit = False\n",
        "\n",
        "# 4-bit quantization\n",
        "compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=USE_NESTED_QUANT,\n",
        ")\n",
        "\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL,\n",
        "        load_in_8bit=load_in_8bit,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=device_map,\n",
        "        use_cache=False,  # We will be using gradient checkpointing\n",
        "        trust_remote_code=True,\n",
        "        use_flash_attention_2=True,\n",
        ")"
      ],
      "metadata": {
        "id": "bUCzoXs5sIHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "1I_c7ilAsJmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up lora\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=LORA_TARGET_MODULES.split(\",\"),\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "cYA56GxDsMLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.start_iteration = 0\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"aadhafun/{OUTPUT_DIR}\",\n",
        "    dataloader_drop_last=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    max_steps=MAX_STEPS,\n",
        "    eval_steps=EVAL_FREQ,\n",
        "    save_steps=SAVE_FREQ,\n",
        "    logging_steps=LOG_FREQ,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LR,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    warmup_steps=NUM_WARMUP_STEPS,\n",
        "    gradient_accumulation_steps=GR_ACC_STEPS,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=FP16,\n",
        "    bf16=BF16,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    push_to_hub=True,\n",
        "    include_tokens_per_second=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Gx9H1jxIsNsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "print(\"Training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "NGaLqZRysP2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the original model first\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL,\n",
        "    quantization_config=None,\n",
        "    device_map=None,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ").cuda()\n",
        "\n",
        "# merge fine-tuned weights with the base model\n",
        "peft_model_id = f\"Your_HF_username/{OUTPUT_DIR}\"\n",
        "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
        "model.merge_and_unload()"
      ],
      "metadata": {
        "id": "BjrjuYUcsY0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_code_completion(prefix, suffix):\n",
        "    text = prompt = f\"\"\"{prefix}{suffix}\"\"\"\n",
        "    model.eval()\n",
        "    outputs = model.generate(\n",
        "        input_ids=tokenizer(text, return_tensors=\"pt\").input_ids.cuda(),\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.2,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.0,\n",
        "    )\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "idzv9B-xsa3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"\"\"from peft import LoraConfig, TaskType, get_peft_model\n",
        "from transformers import AutoModelForCausalLM\n",
        "peft_config = LoraConfig(\n",
        "\"\"\"\n",
        "suffix =\"\"\"\"\"\"\n",
        "\n",
        "print(get_code_completion(prefix, suffix))"
      ],
      "metadata": {
        "id": "2vDnO4djsc02"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}